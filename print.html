<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>CAS CS 320: Principles of Programming Languages</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="preface.html">Preface</a></li><li class="chapter-item expanded "><a href="intro.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="grammar-intro.html"><strong aria-hidden="true">2.</strong> Formal Grammar</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="grammar-bnf.html"><strong aria-hidden="true">2.1.</strong> Backus-Naur Form</a></li><li class="chapter-item expanded "><a href="grammar-ambiguity.html"><strong aria-hidden="true">2.2.</strong> Ambiguity</a></li><li class="chapter-item expanded "><a href="grammar-ebnf.html"><strong aria-hidden="true">2.3.</strong> Extended BNF</a></li><li class="chapter-item expanded "><a href="grammar-regular.html"><strong aria-hidden="true">2.4.</strong> Regularity</a></li></ol></li><li class="chapter-item expanded "><a href="parsing.html"><strong aria-hidden="true">3.</strong> Parsing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="parsing-grammar.html"><strong aria-hidden="true">3.1.</strong> Grammars as ADTs</a></li><li class="chapter-item expanded "><a href="parsing-combinators.html"><strong aria-hidden="true">3.2.</strong> Parser Combinators</a></li></ol></li><li class="chapter-item expanded "><a href="semantics.html"><strong aria-hidden="true">4.</strong> Formal Semantics</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="semantics-op.html"><strong aria-hidden="true">4.1.</strong> Operational Semantics</a></li></ol></li><li class="chapter-item expanded "><a href="variables.html"><strong aria-hidden="true">5.</strong> Variables</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="variables-scoping.html"><strong aria-hidden="true">5.1.</strong> Scoping</a></li><li class="chapter-item expanded "><a href="variables-binding.html"><strong aria-hidden="true">5.2.</strong> Binding</a></li></ol></li><li class="chapter-item expanded "><a href="subprograms.html"><strong aria-hidden="true">6.</strong> Subprograms</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="subprograms-ar.html"><strong aria-hidden="true">6.1.</strong> Activation Records</a></li><li class="chapter-item expanded "><a href="subprograms-as.html"><strong aria-hidden="true">6.2.</strong> Activation Stack</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">CAS CS 320: Principles of Programming Languages</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="preface"><a class="header" href="#preface">Preface</a></h1>
<p>These are notes for the course <strong>CAS CS 320: Principles of Programming Languages (Spring 2024)</strong> taught at Boston University.
They are based on material created by several members of the <em>Principles of Programming and Verification (POPV)</em> group, perhaps most notably Marco Gaboardi and Hongwei Xi.</p>
<p>CS 320 has recently been taught in two parts.
The first part is on the basics of OCaml, using the now mainstay text <a href="https://cs3110.github.io/textbook/cover.html">OCaml Programming: Correct + Efficient + Beautiful</a> by Michael R. Clarkson.
During the second part, students learn the fundamentals necessary to build a stack-based interpreter.
This part of the course has no accompanying text, in part because material on the topic tends to go into either too much or too little detail.
These notes attempt to fill this gap, to give students a reference for the second half of the course.</p>
<p>This year (Spring 2024) is the first year we are using these notes.
This is to say: <strong>they are incredibly incredibly rough.</strong>
There are likely rife with typos, humorous misspellings and grammatical errors, poorly written sections, missing explanations, and the like.
To students: we appreciate your (assumed) patience.
To everyone: any comments are welcome.
If you spot a error, you are encouraged to post an issue to the associated <a href="https://github.com/nmmull/CS320-Notes">GitHub repository</a>.</p>
<p>These notes are generated using <a href="https://rust-lang.github.io/mdBook/">mdbook</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>When we go to write a program in our favorite programming language (OCaml, I presume) we fill a file with a bunch of symbols and text.
This is one of the beauties of programming: looking past the bells and whistles provided by editors and IDEs, a program is just a stream of characters.</p>
<p>At some point in our programming workflow (hopefully not the very end) we want to verify that what we've written thus far actually works, so we <em>run</em> our program.
In some IDEs, this literally means pressing a play button.
For more down-to-earth setups, this might mean opening up a terminal and typing out a few commands.</p>
<p>In either case, we are running a <em>different</em> program in order to run the program we've written (huh).
Our goal is to understand what's going on here: <em>What is this program doing? How does it do it? What sorts of data structures does this program use? etc., etc.</em></p>
<p>Our intuition should probably tell us this program is doing something fairly complicated.
Just imagine how hard it is for us as humans (and, more specifically, students) to follow directions.
How often do we find ourselves wishing we had read <em>all</em> the instructions before starting on some task (like we we're told to do)?</p>
<p><strong>So, our basic question:</strong> <em>How do we get from that stream of symbols to the <strong>output</strong> of our program?</em></p>
<h2 id="interpretation-vs-compilation"><a class="header" href="#interpretation-vs-compilation">Interpretation vs. Compilation</a></h2>
<p>We start by making a loose but ultimately important distinction, that of <em>interpretation</em> versus <em>compilation</em>.
At the risk of oversimplicating the issue, we will use the following definitions to distinguish between these two processes.</p>
<blockquote>
<p><strong>Interpretation</strong> is the process of of taking a source program and its input, and then running it immediately to get its output.</p>
</blockquote>
<p>In the case of compilation we're not immediately interested in the output of our program, but rather punt that concern to a different process.</p>
<blockquote>
<p><strong>Compilation</strong> is the process of taking a source program in a high-level language and then <em>translating it</em> into a program in a low-level language which can given input and run separately.</p>
</blockquote>
<p>These processes are often represented by the following ubiquitous (verging on patronizingly) simple diagrams.</p>
<p><img src="images/interp.png" alt="Compilation" /></p>
<p>Again, this is a loose distinction, e.g., there may be intermediate translations in the process of intepretation.
We will look briefly at compilation towards the end of the course, but we start by focusing on interpretation.</p>
<h2 id="pure-interpretation-pipeline"><a class="header" href="#pure-interpretation-pipeline">Pure Interpretation Pipeline</a></h2>
<p>So, <em>what's happening inside an interpreter?</em>
We start (as is typically the case) with a high-level diagram.
We can think of this image as providing a window into the "Interpreter" box above.</p>
<p><img src="images/interp-inner.png" alt="Interpreter Pipeline" /></p>
<p>A lot to parse here (pun intended), but this also gives us an outline of what is to come in the remainder of these notes:</p>
<ul>
<li>
<p>The first two boxes turn that sequences of characters we've written into something that is easy to evaluate, easy to "run".
This part of intepretation is about the <em>form</em> (or <em>syntax</em>) of the program.
We use <strong>Formal Grammars</strong> (Chapter 2) to represent this form, i.e., to describe the result of <strong>Parsing</strong> (Chapter 3).
Just like natural language, programming languages are heirarchical, and representing that heirarchical structure explicitly will make it easier to evaluate the program.</p>
</li>
<li>
<p>The next two boxes take the heirarchical structure gotten by analyzing the syntax of our program, and then evaluate it (run it) to get its output (possibly building an intermediate representation).
This part of intperetation is about the <em>meaning</em> (or <em>semantics</em>) of the program.
We use <strong>Formal Semantics</strong> (Chapter 4) to describe what it means to give a program this meaning.
This is where we need to concern ourselves with how <strong>Variables</strong> (Chapter 5) are handled and how function calls are made (i.e., how <strong>Subprograms</strong> (Chapter 6) are run).</p>
</li>
</ul>
<p>As we will see, some of these steps are skipped (with parser combinators, it is often possible to perform lexical analysis alongside parsing) or added to (by the end of the course, we will include a compilation step as part of semantic analysis).
The hope is just that we are starting to think more carefully about what is going on when our programs are run.</p>
<h2 id="whats-next"><a class="header" href="#whats-next">What's Next</a></h2>
<p>Programming languages form is one of these topics that has incredibly deep implication both in theory and in practice.
It will be important to us not just to think about how to implement an interpreter, but also about the underlying logical frameworks we need for programming languages to make sense.</p>
<p>So there will be a frequent give-and-take here between theory and practice.
Be prepared to do handwritten assignments (not quite proofs, but something akin to them) as well as programming assignments (i.e., building the components interpreter).</p>
<p>We begin with the study of <strong>Formal Grammar</strong>, the basis of parsing (and also of theoretical linguistics).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="formal-grammar"><a class="header" href="#formal-grammar">Formal Grammar</a></h1>
<p>Most of us are likely familiar with grammar in the context of natural language.
In a primary school English class, we might learn that we should use the artical "an" instead of "a" if its corresponding noun starts with a vowel sound, or that what follows a semicolon should be a independent clause (i.e., should be able to stand on its own as a complete sentences).
These are examples of English <em>grammar rules</em>.</p>
<p>Grammar, in broad strokes, refers to the rules which govern what constitutes a well-formed sentence in a given language, barring low-level syntactic concerns like spelling or white space.
It is the concern of grammar to determine that</p>
<pre><code>I taught the car in the refrigerator
</code></pre>
<p>makes grammatical sense and that</p>
<pre><code>I car teach refrigerator in there
</code></pre>
<p>does not.
It is <em>not</em> the concern of grammar to determine that the first sentence, though grammatical, has no reasonable interpretation in English (except, perhaps, in surrealist fiction).</p>
<p>Programming languages─being themselves languages in their own right, albeit more stringent ones than natural languages─have their own grammars, i.e., rules for determining what counts as a well-formed <em>program</em>.
Due to the grammatic precision expected of programming languages (we don't want too much variation in what we are allowed to write as a valid program), these tend to be called <em>formal grammars</em>.</p>
<blockquote>
<p><strong>Example.</strong> In OCaml, the program</p>
<pre><code class="language-ocaml">let f x = x + 1
</code></pre>
<p>is well-formed, but the program</p>
<pre><code class="language-ocaml">let f x = x 1 +
</code></pre>
<p>is not, because the rule for using the <code>+</code> operator is that its arguments appear to its left and its right.
That is, it is an <em>infix operator</em> (it can be used as a prefix operator if put in parentheses, e.g. <code>(+) x 1</code>, but it cannot in any circumstances be used as a postfix operator).
We will discuss fixity in more detail later.</p>
</blockquote>
<p>As in the case of natural language, grammars for programming languages are not concerned with the <em>meaning</em> of programs, just their well-formedness. The program</p>
<pre><code>let omega x = x x
</code></pre>
<p>is well-formed in OCaml, but it does not type-check since the argument <code>x</code> is expected to be a function of type <code>'a -&gt; 'b</code> as well as an argument of type <code>'a</code>, an impossibility in the type system of OCaml.</p>
<p>If our goal is to interpret computer programs, then we have to understand formally─both theoretically and practically─the grammars describing the well-formed programs in those languages.
This means being able to represent and interpret representations of formal grammars.
The grammar of OCaml, for example, is given in its entirety in <a href="https://v2.ocaml.org/manual/expr.html">The OCaml Manual</a>.
After going through this chapter, you should be able to intepret the specification given there.</p>
<p>Placing this in the pipeline of interpretation we discussed in the previous chapter, grammar is used to represent the output of parsing (which we will take up in the following chapter).
As a reminder, a stream of tokens is parsed into a <em>parse tree</em>, a hierarchical structure which describes the way the program is formally composed.
As we mentioned before (and as we will come to later understand) it is easier to determine the meaning of a program (i.e., to interpret it) given its hierarchical structure as opposed to its linear form as a stream of tokens.</p>
<blockquote>
<p><strong>Remark.</strong> This is another way of conceptualizing the role of grammar: it determines the hierarchical structure of a sentence.
A sentence may be considered well-formed if it can constructed as well-formed parse tree, e.g.</p>
<p><img src="images/parse-tree.png" alt="Parse Tree for the above English sentence." /></p>
</blockquote>
<p>Its not important that you know/remember exactly what you each of the abbreviations in the above image stand for (this is not a linguistics course or an English grammar course) but hopefully the structure aligns with your intuition about how words in the sentences are grouped.</p>
<h2 id="chapter-summary"><a class="header" href="#chapter-summary">Chapter Summary</a></h2>
<p>In what follows, we will:</p>
<ul>
<li>define <em>Backus-Naur Form</em> specifications, a way of describing so-called <em>context-free grammars</em>, which we will use to present the grammars of programming languages;</li>
<li>discuss <em>ambiguity</em> in grammar along with how to avoid it (and why);</li>
<li>take a brief detour to talk about <em>regular grammars</em> and <em>regular expressions</em>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="backus-naur-form"><a class="header" href="#backus-naur-form">Backus-Naur Form</a></h1>
<p>Backus-Naur Form (BNF) specifications are used to describe what are called <em>context-free grammars</em>.
Context-free grammars form a class of formal grammars which are sufficiently expressive to capture the grammars of most programming languages.
We will be using BNF specifications to describe the rules which determine well-formed programs in programming languages we aim to interpret.</p>
<p>First, a toy example/thought experiment.
Consider again the following English statement.</p>
<pre><code>the cow jumped over the moon
</code></pre>
<p>Suppose we tried to break down the cognitive process of determining that this sentence is grammatical.
We might first recognize that each word falls into a particular part of speech.
We can represent this step of the process by replacing each word in the sentence with a symbol <em>standing for</em> each figure of speech (the choice of symbol being influenced by what is to come).</p>
<pre><code>&lt;article&gt; &lt;noun&gt; &lt;verb&gt; &lt;prep&gt; &lt;article&gt; &lt;noun&gt;
</code></pre>
<p>We then might recognize some familiar patterns: <code>&lt;article&gt; &lt;noun&gt;</code> captures the determination or quantification of an object, so we might mentally group these symbols (into what grammaticists call <em>nominal phrases</em> or <em>noun phrases</em>) and represent them by a new symbol:</p>
<pre><code>&lt;noun-phrase&gt; &lt;verb&gt; &lt;prep&gt; &lt;noun-phrase&gt;
</code></pre>
<p>Then we might recognize that a preposition followed by a noun phrase is also single unit ("over the moon", "through the woods", and "behind the wall" are examples of <em>prepositional phrases</em>) so that the structure of the entire sentence may be represented as</p>
<pre><code>&lt;noun-phrase&gt; &lt;verb&gt; &lt;prep-phrase&gt;
</code></pre>
<p>Then we might recognize that prepositional phrases can modify verbs, again creating a single unit (e.g., "ran to the car", "arose from bed") leaving us with something like</p>
<pre><code>&lt;noun-phrase&gt; &lt;verb-phrase&gt;
</code></pre>
<p>which we should finally recognize the canonical structure of a well-formed sentence: <em>a thing does a thing.</em>
A bit hand-wavy, but hopefully we can see that this accounts roughly for what we do when we judge that the above sentence is grammatical.</p>
<p>Putting these steps in reverse order (and starting with a single symbol <code>&lt;sentence&gt;</code>, for reasons we will see below) we get something that looks like a <em>proof</em> or <em>evidence</em> that <code>the cow jumped over the moon</code> is a grammatical sentence.</p>
<pre><code>&lt;sentence&gt;
&lt;noun phrase&gt;    &lt;verb phrase&gt;
&lt;noun phrase&gt;    &lt;verb&gt; &lt;prep phrase&gt;
&lt;noun phrase&gt;    &lt;verb&gt; &lt;prep&gt; &lt;noun phrase&gt;
&lt;article&gt; &lt;noun&gt; &lt;verb&gt; &lt;prep&gt; &lt;article&gt; &lt;noun&gt;
the       cow    jumped over   the       moon
</code></pre>
<p>That is, a representation of our congnitive process.
And if we squint, we can see something that hiearchical, something that looks a bit like the parse tree in the introduction to this chapter.</p>
<p><img src="images/parse-tree-2.png" alt="Another parse tree" /></p>
<p>A <em>formal grammar</em> is meant to model this cognitive process of classifying a sentence as grammatical by verifying that it has the "right" hierarchical structure.</p>
<h2 id="definitions"><a class="header" href="#definitions">Definitions</a></h2>
<p>In defining a formal grammar, we have to fix ourselves to a collection of symbols.
These symbols are divided into two disjoint groups: the <strong>terminal symbols</strong> and the <strong>non-terminal symbols</strong>.
In what follows (and as above) we will always notate a non-terminal symbol by something of the form <code>&lt;non-term&gt;</code> (where we replace <code>non-term</code> with something more descriptive) and terminal symbols by sequence of (typically) alphanumeric symbols.</p>
<blockquote>
<p><strong>Remark.</strong> We almost never state outright what the underlying symbols of a grammar are.
It should always be possible to determine what terminal and non-terminal symbols we are considering by looking at the BNF specification itself.</p>
</blockquote>
<p>In the "proof" that we gave that <code>the cow jumped over the moon</code> was grammatical, we built a sequence of not-quite sentences, until the very last one which was an actual sentences. We call these not-quite sentences <em>sentential forms</em>.</p>
<blockquote>
<p><strong>Definition.</strong>
A <strong>sentential form</strong> is a sequence of symbols (terminal or non-terminal).
A <strong>sentence</strong> is a sequence of terminal symbols.</p>
</blockquote>
<p>We notate a sequences of symbols by white space separation.
For example, <code>the dog jumped</code> is a sentence and <code>the &lt;noun&gt; jumped</code> is a sentential form.
But it is important to note that <em>this is just notation</em>.
If it helps, it may be useful to imagine <code>[the, &lt;noun&gt;, jumped]</code> when thinking about what a sentential form is.</p>
<p>In the (reversed) process of building sentential forms, we replaced non-terminal symbols with sentential forms, e.g., we replaced <code>&lt;noun phrase&gt;</code> with <code>&lt;article&gt; &lt;noun&gt;</code>.
A grammar is determined by what replacements we are allowed to do.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>production rule</strong> is an equation of the form</p>
<pre><code>&lt;non-term&gt; ::= SENTENTIAL-FORM
</code></pre>
<p>where the left-hand side of the <code>::=</code> is a non-terminal symbol, and the right-hand side is a sentential form.</p>
</blockquote>
<p>We read a production rule as saying: "the non-terminal symbol on the left-hand side can be replaced with the sentential form on the right hand side."
In a sense, production rules, <em>define</em> the non-terminal symbols: e.g., a sentence is a noun phrase followed by a verb phrase.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>BNF specification</strong> is a collection of production rules, together with a designated the <strong>starting symbol</strong>.</p>
</blockquote>
<p>In these notes, the start symbol will be designated as the left-hand side of the <em>first</em> rule appearing in a specification.
The following is an example of a grammar which we will show to <em>recognize</em> the sentence above.</p>
<pre><code>&lt;sentence&gt;    ::= &lt;noun-phrase&gt; &lt;verb-phrase&gt;
&lt;verb-phrase&gt; ::= &lt;verb&gt; &lt;prep-phrase&gt;
&lt;verb-phrase&gt; ::= &lt;verb&gt;
&lt;prep-phrase&gt; ::= &lt;prep&gt; &lt;noun-phrase&gt;
&lt;noun-phrase&gt; ::= &lt;article&gt; &lt;noun&gt;
&lt;article&gt;     ::= the
&lt;noun&gt;        ::= cow
&lt;noun&gt;        ::= moon
&lt;verb&gt;        ::= jumped
&lt;prep&gt;        ::= over
</code></pre>
<p>Note that a non-terminal symbol can have multiple associated production rules.
This is common enough that we have special syntax for this.</p>
<blockquote>
<p><strong>Notation.</strong> We will write</p>
<pre><code>&lt;non-term&gt; ::= SENT-FORM-1 | SENT-FORM-2 | ... | SENT-FORM-n
</code></pre>
<p>as shorthand for</p>
<pre><code>&lt;non-term&gt; ::= SENT-FORM-1
&lt;non-term&gt; ::= SENT-FORM-2
...
&lt;non-term&gt; ::= SENT-FORM-n
</code></pre>
</blockquote>
<p>With this shorthand, we can simply the above grammar:</p>
<pre><code>&lt;sentence&gt;    ::= &lt;noun-phrase&gt; &lt;verb-phrase&gt;
&lt;verb-phrase&gt; ::= &lt;verb&gt; | &lt;verb&gt; &lt;prep-phrase&gt;
&lt;prep-phrase&gt; ::= &lt;prep&gt; &lt;noun-phrase&gt;
&lt;noun-phrase&gt; ::= &lt;article&gt; &lt;noun&gt;
&lt;article&gt;     ::= the
&lt;noun&gt;        ::= cow | moon
&lt;verb&gt;        ::= jumped
&lt;prep&gt;        ::= over
</code></pre>
<p>The last piece of the thought experiment above is the "proof" that the given sentence was grammatical.
We codify this in the notion of a <em>derivation</em>.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>derivation</strong> of a sentence <code>S</code> in a BNF grammar is a sequence of sentential forms with the following properties:</p>
<ul>
<li>it beginning with the designated start symbol;</li>
<li>it ends in the sentence <code>S</code>;</li>
<li>each sentential form is a the result of replacing <em>one of</em> the non-terminal symbols in the preceding sentence with a sentential form according to a production rule of the grammar.</li>
</ul>
<p>We say that a grammar <strong>recognizes</strong> a sentence <code>S</code> if there is a derivation of <code>S</code> in the grammar.</p>
</blockquote>
<p>A bit of a mouthful, but this essentially restates the process from the thought experiment in a formal way.
That said, it deviates in one way which makes the definition easier to state: in the thought experiment, we allowed ourselves to replace multiple non-terminal symbols simultaneously.
This is not allowed in the above notion of a derivation. A "correct" derivation (correct according to the above definition) would look like:</p>
<pre><code>&lt;sentence&gt;!
&lt;noun-phrase&gt;!     &lt;verb-phrase&gt;
&lt;noun-phrase&gt;      &lt;verb&gt;  &lt;prep-phrase&gt;!
&lt;noun-phrase&gt;!     &lt;verb&gt;  &lt;prep&gt;  &lt;noun-phrase&gt;
&lt;article&gt;  &lt;noun&gt;  &lt;verb&gt;  &lt;prep&gt;  &lt;noun-phrase&gt;!
&lt;article&gt;! &lt;noun&gt;  &lt;verb&gt;  &lt;prep&gt;  &lt;article&gt;  &lt;noun&gt;
the        &lt;noun&gt;! &lt;verb&gt;  &lt;prep&gt;  &lt;article&gt;  &lt;noun&gt;
the        cow     &lt;verb&gt;! &lt;prep&gt;  &lt;article&gt;  &lt;noun&gt;
the        cow     jumped  &lt;prep&gt;! &lt;article&gt;  &lt;noun&gt;
the        cow     jumped  over    &lt;article&gt;! &lt;noun&gt;
the        cow     jumped  over    the        &lt;noun&gt;!
the        cow     jumped  over    the        moon
</code></pre>
<p>For emphasis I've appended an exclamation point to the non-terminal symbol which is replaced at each step (this is <strong>just</strong> for emphasis, they are <strong>not</strong> a part of the derivation, and will not be included in latter derivations).</p>
<p>This derivation also indicates that there are many possible derivations.</p>
<blockquote>
<p><strong>Definition.</strong>
A <strong>leftmost derivation</strong> of a sentence is one in which the leftmost nonterminal symbol is expanded in each step.
A <strong>rightmost derivation</strong> is one in which the rightmost nonterminal symbol is expanded in each step.</p>
</blockquote>
<p>Note that the above derivation is neither the leftmost derivation or the rightmost derivation.</p>
<blockquote>
<p><strong>Exercise.</strong>
Write leftmost and rightmost derivations for the sentence <code>the cow jumped over the moon</code> in the above grammar.</p>
</blockquote>
<h2 id="a-more-interesting-example"><a class="header" href="#a-more-interesting-example">A More Interesting Example</a></h2>
<p>The following is a BNF specification for a fragment of a simple imperative programming language.</p>
<pre><code>&lt;program&gt; ::= &lt;stmts&gt;
&lt;stmts&gt;   ::= &lt;stmt&gt; | &lt;stmt&gt; ; &lt;stmts&gt;
&lt;stmt&gt;    ::= &lt;var&gt; = &lt;stmt&gt;
&lt;var&gt;     ::= a | b | c | d
&lt;expr&gt;    ::= &lt;term&gt; | &lt;term&gt; + &lt;term&gt; | &lt;term&gt; - &lt;term&gt;
&lt;term&gt;    ::= &lt;var&gt; | const
</code></pre>
<p>In English, we would read this specification as:</p>
<blockquote>
<p>A <em>program</em> is a <em>sequence of statements</em>.
A <em>sequence of statements</em> is either a <em>single statement</em>, or a <em>single statement</em> followed a semicolon, followed by a <em>sequence of statements</em>...</p>
</blockquote>
<p>And so on.
This second rule highlights something interesting which we can do in BNF specifications: rules are allowed to be <em>recursive</em>.
The production rule for <code>&lt;stmts&gt;</code> allows us to replace it with a sentential form which <em>contains</em> the non-terminal symbol <code>&lt;stmts&gt;</code>.
This is quite powerful, particularly because it means it is possible to derive an infinite number of sentences in a given grammar.</p>
<blockquote>
<p><strong>Exercise.</strong> Determine the number of sentences that can be derived in the grammar for sentences above (i.e., the number of sentences which can be derived from <code>&lt;sentence&gt;</code>).</p>
</blockquote>
<p>Consider the following program.</p>
<pre><code>a = const ;
a = a + const ;
b = a
</code></pre>
<p>We can verify that this program is recognized by the above grammar by finding a (leftmost) derivation.</p>
<pre><code>&lt;program&gt;
&lt;stmts&gt;
&lt;stmt&gt; ; &lt;stmts&gt;
&lt;var&gt; = &lt;expr&gt; ; &lt;stmts&gt;
a = &lt;expr&gt; ; &lt;stmts&gt;
a = &lt;term&gt; ; &lt;stmts&gt;
a = const ; &lt;stmts&gt;
a = const ; &lt;stmt&gt; ; &lt;stmts&gt;
a = const ; &lt;var&gt; = &lt;expr&gt; ; &lt;stmts&gt;
a = const ; a = &lt;expr&gt; ; &lt;stmts&gt;
a = const ; a = &lt;term&gt; + &lt;term&gt; ; &lt;stmts&gt;
a = const ; a = &lt;var&gt; + &lt;term&gt; ; &lt;stmts&gt;
a = const ; a = a + &lt;term&gt; ; &lt;stmts&gt;
a = const ; a = a + const ; &lt;stmts&gt;
a = const ; a = a + const ; &lt;var&gt; = &lt;expr&gt;
a = const ; a = a + const ; b = &lt;expr&gt;
a = const ; a = a + const ; b = &lt;term&gt;
a = const ; a = a + const ; b = &lt;var&gt;
a = const ; a = a + const ; b = a
</code></pre>
<blockquote>
<p><strong>Remark.</strong> As a reminder, we're not interested in white space when we consider whether or not a sentence is recognized by a grammar.
The choice to present the sentences in three lines was for readability, and the choice to present it in a single line in the derivation was for convenience.</p>
</blockquote>
<p>It may also be worthwhile to point out a feature of the last three four lines of the above derivation: even if a nonterminal symbol is replaced by a <em>single</em> nonterminal symbol in succession, <em>we have to include each step</em>.
We're only allowed to apply one production rule at a time, e.g., we cannot immedatiely replace <code>&lt;expr&gt;</code> with <code>&lt;var&gt;</code> because that is not one of our production rules.</p>
<blockquote>
<p><strong>Exercise.</strong> Does the above program have a rightmost derivation? Why or why not?</p>
</blockquote>
<blockquote>
<p><strong>Exercise.</strong> Verify that</p>
<pre><code>a = a + a ; b = b
</code></pre>
<p>is recognized by the above grammar.</p>
</blockquote>
<h2 id="parse-trees"><a class="header" href="#parse-trees">Parse Trees</a></h2>
<p>Grammars imbue sentences with hierarchical structure.
This structure is represented graphically as a <em>parse tree</em>.
We've seen a couple examples of English grammar parse trees so far, but we can also build parse trees for sentences recognized by <em>any</em> grammar with a BNF specification.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>parse tree</strong> for a sentence <code>S</code> in a grammar is a (ordered) tree <code>T</code> with the following properties:</p>
<ul>
<li>every leaf of <code>T</code> has a terminal symbol;</li>
<li>every non-leaf node <code>n</code> has a nonterminal symbol (we write <code>val(n)</code> for the value at <code>n</code>);</li>
<li>if a node <code>n</code> with has children <code>[t1, t2, ..., tk]</code> then
<pre><code>val(n) ::= root(t1) root(t2) ... root(tk)
</code></pre>
is a production rule in the grammar (where <code>root(t)</code> denotes the value at the root of the tree <code>t</code>);</li>
<li>The leaves (in order) (i.e., the <em>frontier</em> of <code>T</code>) form the sentence <code>S</code>.</li>
</ul>
</blockquote>
<p>The details of the above definition are not important, as long as you have the right picture in your head.
For example, the sentence <code>a = b + const</code> has the following derivation.</p>
<pre><code>&lt;program&gt;
&lt;stmts&gt;
&lt;stmt&gt;
&lt;var&gt; = &lt;expr&gt;
a = &lt;expr&gt;
a = &lt;term&gt; + &lt;term&gt;
a = &lt;var&gt; + &lt;term&gt;
a = b + &lt;term&gt;
a = b + const
</code></pre>
<p>And has the following parse tree.</p>
<p><img src="images/parse-tree-3.png" alt="A simple parse tree" /></p>
<blockquote>
<p><strong>Exercise.</strong> Given the ADT</p>
<pre><code class="language-ocaml">type 'a tree
   = Leaf of 'a
   | Node of 'a * 'a tree list
</code></pre>
<p>Write the OCaml function <code>frontier</code> which, given</p>
<ul>
<li><code>t</code> : <code>'a tree</code></li>
</ul>
<p>returns the list of leaves of <code>t</code> in order from left to right.</p>
</blockquote>
<p>Every derivation can be converted into a parse tree, and vice versa, but multiple derivations may correspond to the same parse tree.
This will be important when we cover ambiguity in the next section.</p>
<blockquote>
<p><strong>Exercise.</strong> Write a derivation corresponding to the above parse tree when is neither leftmost nor rightmost.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ambiguity"><a class="header" href="#ambiguity">Ambiguity</a></h1>
<p>As participants of language, we are no strangers to grammatical ambiguity.
Take, for instance, the following sentence I've unapologetically snarfed from the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Syntactic_ambiguity">Syntactic Ambiguity</a>.</p>
<pre><code>John saw the man on the mountain with the telescope
</code></pre>
<p>Was John using the telescope?
Was the man carrying the telescope?
Are the multiple mountains, one of which has a telescope on it?</p>
<p>The ambiguity exists because it is not clear what hierarchical structure should scaffold this sentence.
Here are two options for the verb phrase in the sentence.</p>
<p><img src="images/ambiguous-1.png" alt="Option 1" /></p>
<p>Again, not a linguistic class, but it is enough to recognize that <code>with the telescope</code> is grouped alongside the verb phrase starting with <code>saw</code>, indicating that John was <em>using</em> the telescope.
The other option (though not the <em>only</em> other option):</p>
<p><img src="images/ambiguous-2.png" alt="Option 2" /></p>
<p>The prepositional phrase <code>with the telescope</code> is grouped with <code>the man on the mountain</code>, indicating the man was <em>carrying</em> the telescope.</p>
<p>The ambiguity comes from not being completely sure which parse tree to give to the sentence.
That is, it comes from the fact that we have to experience language in a linear fashion, either by reading it or hearing it.
If our interlocutor could <em>display</em> the parse tree of their statement (floating eerily in space before your eyes) there would be nothing to say of (grammatical) ambiguity.</p>
<p>But this is not how we experience language (or how we write programs, for that matter).
To drive the point home, there is a natural-enough looking grammar which recognizes the above sentence.</p>
<pre><code>&lt;s&gt;  ::= &lt;np&gt; &lt;vp&gt;
&lt;vp&gt; ::= &lt;v&gt; | &lt;v&gt; &lt;np&gt; | &lt;v&gt; &lt;np&gt; &lt;pp&gt;
&lt;pp&gt; ::= &lt;p&gt; &lt;np&gt;
&lt;np&gt; ::= &lt;n&gt; | &lt;d&gt; &lt;n&gt; | &lt;np&gt; &lt;pp&gt;
&lt;n&gt;  ::= John | man | mountain | telescope
&lt;v&gt;  ::= saw
&lt;d&gt;  ::= the
&lt;p&gt;  ::= on | with
</code></pre>
<p>The ambiguity comes exactly from the fact that this grammar has two parse trees which recognize the same sentence; we don't know what structure to give the sentence.
Or equivalently, since parse trees correspond to exactly one leftmost derivation (you should try to convince yourself of this), the ambiguity comes from there being multiple leftmost derivations.</p>
<blockquote>
<p><strong>Exercise.</strong> Give two leftmost derivations of <code>John saw the man on the mountain with the telescope</code> in the above grammar.</p>
</blockquote>
<p>Ambiguity in natural language is a complex topic, coming not just from parse-tree ambiguity, but also from the fact that defining a formal grammar for natural language is <em>hard</em> (perhaps impossible).
But, restricted to the context of formal grammars, ambiguity has a wholly unambiguous definition.</p>
<blockquote>
<p><strong>Definition.</strong> A grammar is <strong>ambiguous</strong> there is a sentence it recognizes which has two distinct parse trees.
Equivalently, it is ambiguous of there is a sentence it recognizes which has two distinct leftmost derivations.</p>
</blockquote>
<p>Thus, the above grammar is ambiguous in the formal sense of the word.</p>
<p>More to the point, consider the following grammar which may be seen as a prototype of a grammar for arithmetic expressions (something we will probably want if we're to give a grammar for a programming language).</p>
<pre><code>&lt;expr&gt; ::= &lt;var&gt; | &lt;expr&gt; &lt;op&gt; &lt;expr&gt;
&lt;op&gt;   ::= + | - | * | /
&lt;var&gt;  ::= x
</code></pre>
<p>This seems, ignoring obvious issues, a reasonable enough definition; <em>an expression is either a variable or a pair of expressions with an operator between them</em>.
Note that the recursive nature of the first production rule means that this grammar recognizes an infinite number of sentence.</p>
<blockquote>
<p><strong>Exercise.</strong> Give a leftmost a derivation of <code>x * x + x * x</code> in the above grammar.</p>
</blockquote>
<p>But, with regards to ambiguity, we should already be wary of this grammar, in particular the first production rule.
As soon as we've applied the (second alternative of) the first production rule twice we get:</p>
<pre><code>&lt;expr&gt;
&lt;expr&gt; &lt;op&gt; &lt;expr&gt;
&lt;expr&gt; &lt;op&gt; &lt;expr&gt; &lt;op&gt; &lt;expr&gt;
</code></pre>
<p>For the third line, <em>which of the two <code>&lt;expr&gt;</code> symbols did we expand?</em>
To make this concrete, here are two parse trees for the sentence <code>x + x + x</code>.</p>
<p><img src="images/ambig-parse.png" alt="Ambiguous parse trees" /></p>
<p>Corresponding to the following two leftmost derivations (the first for the tree on the left, the second for the tree on the right).</p>
<pre><code>&lt;expr&gt;
&lt;expr&gt; &lt;op&gt; &lt;expr&gt;
&lt;var&gt; &lt;op&gt; &lt;expr&gt;
x &lt;op&gt; &lt;expr&gt;
x + &lt;expr&gt;
x + &lt;expr&gt; &lt;op&gt; &lt;expr&gt;
x + &lt;var&gt; &lt;op&gt; &lt;expr&gt;
x + x &lt;op&gt; &lt;expr&gt;
x + x + &lt;expr&gt;
x + x + &lt;var&gt;
x + x + x

&lt;expr&gt;
&lt;expr&gt; &lt;op&gt; &lt;expr&gt;
&lt;expr&gt; &lt;op&gt; &lt;expr&gt; &lt;op&gt; &lt;expr&gt;
&lt;var&gt; &lt;op&gt; &lt;expr&gt; &lt;op&gt; &lt;expr&gt;
x &lt;op&gt; &lt;expr&gt; &lt;op&gt; &lt;expr&gt;
x + &lt;expr&gt; &lt;op&gt; &lt;expr&gt;
x + &lt;var&gt; &lt;op&gt; &lt;expr&gt;
x + x &lt;op&gt; &lt;expr&gt;
x + x + &lt;expr&gt;
x + x + &lt;var&gt;
x + x + x
</code></pre>
<p>This demonstrates that the above grammar is ambiguous.</p>
<blockquote>
<p><strong>Aside.</strong> In this example, and many of the examples we will see, it will be fairly clear that the grammar is ambiguous.
As students of computer science, we might think it possible that we could write a <em>program</em> that can check for us if a grammar is ambiguous.
Unfortunately, this is impossible (not just very difficult, but <em>impossible</em>).
This is to say that determining if a context-free grammar is ambiguous is <strong>undecidable</strong> (a term worth looking up if this piques your interest).</p>
</blockquote>
<h2 id="avoiding-ambiguity"><a class="header" href="#avoiding-ambiguity">Avoiding Ambiguity</a></h2>
<p>Our next task it to determine how to avoid this ambiguity, but first, <em>why should we care?</em>
Natural language is ambiguous and we get along perfectly fine.
Why should we go through this trouble to make sure grammars we design are unambiguous?</p>
<p>It's a fair question; the way I see it, it's something of a promise that we make to the user of a programming language that we <strong>never make unspoken assumptions about what a user meant when we read one of their programs.</strong>
To be fair, we try to do this with natural language too, but in communication, if a statement is ambiguous, we can usually just ask our interlocutor what they meant.
We can't do this for a programming language (at least not yet), so instead we make it <em>impossible</em> for a sentence to have multiple meanings.</p>
<blockquote>
<p><strong>Aside.</strong> We see a similar phenomena in legal language, which tends to be grammatically sterile, and usually no fun to read (at least for me).</p>
</blockquote>
<h3 id="reverse-polish-notation"><a class="header" href="#reverse-polish-notation">(Reverse) Polish Notation</a></h3>
<p>If our only concern is avoiding ambiguity, we can use <em>polish notation</em> or <em>reverse polish notation</em>.
In polish notation, operators appear <em>before</em> all their arguments, e.g.,</p>
<pre><code>&lt;expr&gt; ::= &lt;var&gt; | &lt;op&gt; &lt;expr&gt; &lt;expr&gt;
&lt;op&gt;   ::= + | - | * | /
&lt;var&gt;  ::= x
</code></pre>
<p>We won't dwell on this but it turns out this gives us an unambiguous grammar, we don't even need parentheses.</p>
<p>And it's not difficult to imagine what reverse polish notation is: operators always appear <em>after</em> all their arguments.
This is how early calculators (like the <a href="https://en.wikipedia.org/wiki/Hewlett-Packard_9100A">HP 9100A Desktop Calculator</a>, before my time) were designed.
If you wanted to compute something <code>(2 + 3) * (4 - 5)</code>, you would <em>push</em> the values you want to apply operations to onto a <em>stack</em>, and then apply operations to the top elements of the stack, like so:</p>
<pre><code>STACK        RPN EXPRESSION
2            2
2 3          2 3
5            2 3 +
5 4          2 3 + 4
5 4 5        2 3 + 4 5
5 (-1)       2 3 + 4 5 -
(-5)         2 3 + 4 5 - *
</code></pre>
<p>So the sequence of tokens you end up typing into the calculator is an expression in reverse polish notation.</p>
<blockquote>
<p><strong>Exercise.</strong> Derive the sentence <code>+ * x * x - x x x x</code> in the above grammar.</p>
</blockquote>
<p>The obvious issue with (reverse) polish notation is that it is difficult to read.
Imagine working with a language in which if-then-else logic had to be done like:</p>
<pre><code>IFTHENELSE
  cond
  IFTHENELSE
    IFTHENELSE
	  cond
      ifcase
	  elsecase
    ifcase
	elsecase
  elsecase
</code></pre>
<p>This is in part to say that what truly causes ambiguity in expressions is the <em>mixing</em> of operator <em>fixity</em>.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>fixity</strong> of an operator refers to where the (syntactic components of an) operator is placed relative to its arguments.
There are four kinds of operator fixity.</p>
<ul>
<li>A <strong>prefix</strong> operator appears before its arguments. This is like function application in OCaml (e.g., <code>f x</code> or <code>not b</code>).</li>
<li>A <strong>postfix</strong> operator appears after its arguments. This is like type constructor application in OCaml (e.g., <code>int list</code> or <code>bool option list</code>).</li>
<li>An <strong>infix</strong> operator appears in between its arguments. This is like arithmetic operations we learn in primary school (e.g., <code>2 + 3</code> or <code>4 / 5</code>).</li>
<li>A <strong>mixfix</strong> operator is an operator with multiple syntactic components which may appear as some combination of prefix, infix and postfix. This is like if-then-else expressions in OCaml (e.g., <code>if b then x else y</code>).</li>
</ul>
</blockquote>
<p>So if we want to contend with operator fixity (i.e., we don't want <em>just</em> prefix or <em>just</em> postfix operators, as in polish notation or reverse polish notation) then we still have work to do to avoid ambiguity.</p>
<h3 id="parentheses"><a class="header" href="#parentheses">Parentheses</a></h3>
<p>Another simple solution is to surround applications of operations with parentheses:</p>
<pre><code>&lt;expr&gt; ::= &lt;var&gt; | ( &lt;expr&gt; &lt;op&gt; &lt;expr&gt; )
&lt;op&gt;   ::= + | - | * | /
&lt;var&gt;  ::= x
</code></pre>
<p>It then becomes very clear in a derivation like</p>
<pre><code>&lt;expr&gt;
( &lt;expr&gt; &lt;op&gt; &lt;expr&gt; )
( ( &lt;expr&gt; &lt;op&gt; &lt;expr&gt; ) &lt;op&gt; &lt;expr&gt; )
</code></pre>
<p>which <code>&lt;expr&gt;</code> in the second line we expanded.
But we run into a similar issue; <em>lots of parentheses are no fun to read.</em></p>
<blockquote>
<p><strong>Exercise.</strong> Give a derivation of <code>( ( ( x * x ) * x ) + ( x / x ) )</code> in the above grammar.</p>
</blockquote>
<p>So, our real basic question is: <strong>how do we avoid grammar while being able to mix operator fixities and not use so many parentheses?</strong>
And this question has a simple answer in theory: <strong>make explicit assumptions about how operator arguments are grouped.</strong>
This will mean contending with two things: <em>associativity</em> and <em>precedence</em>.</p>
<h2 id="dealing-with-associativity"><a class="header" href="#dealing-with-associativity">Dealing with Associativity</a></h2>
<p>Associativity refers to how arguments are grouped when we are given a sequence of applications of an infix operator in the absence of parentheses, e.g.,</p>
<pre><code>x + x + x + x
</code></pre>
<p>may be understood as any one of the following (among others):</p>
<pre><code>(((x + x) + x) + x)
(x + (x + (x + x)))
((x + x) + (x + x))
(x + ((x + x) + x))
</code></pre>
<blockquote>
<p><strong>Exercise.</strong> Determine the number of ways the expression <code>x + x + x + x</code> can be parenthesized.</p>
</blockquote>
<p>In the case of addition (as we typically understand it) this point is somewhat moot.
The order in which we group arguments does not affect the <em>value</em> of a sequence of additions.
That is, addition is associative (a term whose meaning we may recall from our discussion of list folding).</p>
<blockquote>
<p><strong>Definition.</strong> An operator <code>#</code> is <strong>associative</strong> if</p>
<pre><code>(a # b) # c = a # (b # c)
</code></pre>
<p>for any values <code>a</code>, <code>b</code>, and <code>c</code> for which the above two expressions are defined.</p>
</blockquote>
<p>But not all operators are associative.
Take division for example.
We need to decide how to implicitly parenthesize an expression like <code>a / b / c / d</code>.
Again, we could try to bar the ability to write an expression like this, but we might rather avoid using parentheses if possible.</p>
<blockquote>
<p><strong>Exercise.</strong> How does OCaml evaluate the expression <code>100 / 5 / 4</code>?
How are arguments grouped?</p>
</blockquote>
<p>For binary operators, we typically choose one of the first two choices in the list of parenthesizations above.</p>
<blockquote>
<p><strong>Definition.</strong> A operation <code>#</code> is said to be <strong>left-associative</strong> if sequences of applications of the operator are understood as grouping arguments from the left, i.e.,</p>
<pre><code>a # b # c # d = (((a # b) # c) # d)
</code></pre>
<p>and is said to be <strong>right-associative</strong> if arguments are grouped from the right, i.e.,</p>
<pre><code>a # b # c # d = (a # (b # (c # d)))
</code></pre>
</blockquote>
<blockquote>
<p><strong>Remark.</strong> Another option is that a binary operator can have no associativity.
It does not, for instance, make sense to apply a sequence of <code>=</code> operators in OCaml.</p>
</blockquote>
<p>Bringing this back to grammatical ambiguity, giving a parenthesization of a sequence of operators means specifying a "shape" for the corresponding parse tree.
For example, taking addition to be left-associative, i.e., taking <code>x + x + x</code> to mean <code>((x + x) + x)</code>, implies that, of the two parse trees for this sentence in the above image, the one on the <em>right</em> should be the "correct" one, <strong>not</strong> the one on the left.
So: <em>can we update the grammar so that only the one on right can be constructed?</em></p>
<p>Let's take a simpler case first: consider another grammar, used to generate function types on integers in OCaml without parentheses.</p>
<pre><code>&lt;fun-type&gt; ::= &lt;int-type&gt; | &lt;fun-type&gt; -&gt; &lt;fun-type&gt;
&lt;int-type&gt; ::= int
</code></pre>
<blockquote>
<p><strong>Exercise.</strong> Give two leftmost derivations of <code>int -&gt; int -&gt; int</code> in the above grammar.</p>
</blockquote>
<p>We might recall that function types are right associative, so we understand <code>int -&gt; int -&gt; int</code> to be parenthesized implicitly as <code>int -&gt; (int -&gt; int)</code>.
The problem, as it stands, is that because of the production rule</p>
<pre><code>&lt;fun-type&gt; ::= &lt;fun-type&gt; -&gt; &lt;fun-type&gt;
</code></pre>
<p>the argument type can an arbitrary complex function type.
But we might recognize that, no matter how deep the nesting, the argument types have the special property in the case we assume right associativity: <em>they are it is always just <code>int</code></em>.
Therefore, we might consider the following updated grammar.</p>
<pre><code>&lt;fun-type&gt; ::= &lt;int-type&gt; | &lt;int-type&gt; -&gt; &lt;fun-type&gt;
&lt;int-type&gt; ::= int
</code></pre>
<p>In this grammar, no matter how many times you apply the second alternative of the first production rule, the argument type is always just the <code>int</code> type.
So we were, in fact, able to restrict the shape of the parse tree for sentences, by <em>breaking the symmetry</em> the production rule.</p>
<blockquote>
<p><strong>Exercise.</strong> Write the leftmost derivation of <code>int -&gt; int -&gt; int</code> in the above grammar, along with its parse tree.</p>
</blockquote>
<p>We'll come back to this, but first we have to recognize an issue with the example just given: there is only one operator.
In the presence of multiple operators we have new issues to deal with.
<em>How should <code>x * y + z</code> or <code>x + y - z</code> be implicitly parenthesized?</em></p>
<p>This is an issue of <em>precedence</em> or <em>order of operations</em> something probably already somewhat familiar with without necessarily knowing it.</p>
<h2 id="dealing-with-precedence"><a class="header" href="#dealing-with-precedence">Dealing with Precedence</a></h2>
<p>If you went through the American public school system then you probably learned the abbreviation PEMDAS (<strong>P</strong>arentheses, <strong>E</strong>xponentiation, <strong>M</strong>ultiplication, <strong>D</strong>ivision, <strong>A</strong>ddition, <strong>S</strong>ubtraction, along with an accompanying mnemonic, something like <strong>P</strong>lease <strong>E</strong>xcuse <strong>M</strong>y <strong>D</strong>ear <strong>A</strong>unt <strong>S</strong>ally).
Focusing on just the last four letters for now, this rule tells us that we should group multiplications and divisions first, and then group addition and subtraction.
That is to say, multiplication and division have greater <em>precedence</em> than addition and subtraction.</p>
<blockquote>
<p><strong>Definition.</strong> The <strong>precedence</strong> of an operator, relative to another operator, determines which operator binds more tightly, in the presents of ambiguity.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The expression <code>2 * 3 + 4 * 5</code> should be implicitly parenthesized as <code>((2 * 3) + (4 * 5))</code> because multiplication binds tighter, it is considered first.</p>
</blockquote>
<p>Just as with associativity, the relative precedence of a collection of operators is about determining the shape of the parse tree we get when we generate the sentence with a grammar.
To say that multiplication has higher precedence is to say that when we build the parse tree for <code>x * x + x</code>, we want <code>+</code> to be the top-level operation, at the root of the tree.</p>
<blockquote>
<p><strong>Remark.</strong> One thing that was probably glossed over if/when you learned PEMDAS: what do you do with something like <code>1 + 1 - 1 + 1</code>?
Do you group additions and then subtractions? Or vice versa?
The issue here is that addition and subtraction have the <em>same</em> precedence.
In this case, we will use the associativity of the operators to determine how to parenthesize: <strong>given a sequences of operators of the same precedence, we use their associativity to group them.</strong></p>
<p>Since addition and subtraction are both left-associative, this would be parenthesized as (((1 + 1) - 1) + 1).
Things can get <em>truly</em> complicated if you have two operators with the <em>same</em> precedence, but <em>different</em> associativity.
We will ignore this possibility in this course, but this really matters in languages like Haskell, in which users can define their own operators, with specified precedence and associativity.</p>
</blockquote>
<p>All of this is to say: in order to define an unambiguous grammar without parentheses, we need to know three things of each operator appearing in the grammar: <strong>fixity</strong>, <strong>associativity</strong>, and <strong>precedence</strong>.
We can, for example, present the four basic arithmetic operators along with this information.
It is typical to represent precedence by a positive integer, in which an operator has higher precedence than another if its precedence value is larger.</p>
<div class="table-wrapper"><table><thead><tr><th>Operator</th><th>Fixity</th><th>Associativity</th><th>Precedence</th></tr></thead><tbody>
<tr><td><code>*</code></td><td>infix</td><td>left</td><td>2</td></tr>
<tr><td><code>/</code></td><td>infix</td><td>left</td><td>2</td></tr>
<tr><td><code>+</code></td><td>infix</td><td>left</td><td>1</td></tr>
<tr><td><code>-</code></td><td>infix</td><td>left</td><td>1</td></tr>
</tbody></table>
</div>
<blockquote>
<p><strong>Remark.</strong> This information is also available for all <a href="https://v2.ocaml.org/api/Ocaml_operators.html">OCaml operators</a>.</p>
</blockquote>
<p>With all this, we can now think about how to build an unambiguous grammar for arithmetic expressions.
Just as with associativity, we need to recognize that a couple things:</p>
<ul>
<li>Because of associativity, the right argument of multiplication or division must be a variable <code>x</code>.</li>
<li>Because of precedence, the left argument of multiplication or division must contain only multiplications and divisions.</li>
<li>Because of associativity, the right argument of addition or subtraction must be an expression with only multiplications and divisions.</li>
</ul>
<p>These observations yields the following grammar.
I've tried to use suggestive names to indicate how the three points above manifest.</p>
<pre><code>&lt;expr&gt;         ::= &lt;only-mul-div&gt; | &lt;expr&gt; &lt;add-sub&gt; &lt;only-mul-div&gt;
&lt;only-mul-div&gt; ::= &lt;var&gt; | &lt;only-mul-div&gt; &lt;mul-div&gt; &lt;var&gt;
&lt;add-sub&gt;      ::= + | -
&lt;mul-div&gt;      ::= * | /
&lt;var&gt;          ::= x
</code></pre>
<blockquote>
<p><strong>Exercise.</strong> Give the leftmost derivation of <code>x * x + x * x</code> in the above grammar. Draw its parse tree</p>
</blockquote>
<p>Proving that this grammar is unambiguous is a bit tricky (we won't do this).
It suffices to say that, given we know how to parenthesize such expressions, and this grammar captures the rules we use, it would seem to be ambiguous.</p>
<h2 id="parentheses-again"><a class="header" href="#parentheses-again">Parentheses (Again)</a></h2>
<p>We're not quite done.
Without parentheses, we can't derive all expressions we might want to write down.
Given the rules we've described above, we cannot write an expression (without parentheses) which is equivalent to <code>(x + x) + x</code>.
To give a complete specification of the arithmetic expressions we know and love, we have to add back parentheses (this is the "P" part of PEMDAS).</p>
<p>But where should we put parentheses into the grammar?
As we saw above, if we put them everywhere, we get unnecessarily verbose sentences.
Rather, it should be the case that: <em>it should be possible put <strong>any</strong> expression as an argument to operator if it is wrapped in parentheses.</em>
For this we will replace <code>&lt;var&gt;</code> with something can also be any expression wrapped in parentheses.</p>
<pre><code>&lt;expr&gt;          ::= &lt;only-mul-div&gt; | &lt;expr&gt; &lt;add-sub&gt; &lt;only-mul-div&gt;
&lt;only-mul-div&gt;  ::= &lt;var-or-parens&gt; | &lt;only-mul-div&gt; &lt;mul-div&gt; &lt;var-or-parens&gt;
&lt;add-sub&gt;       ::= + | -
&lt;mul-div&gt;       ::= * | /
&lt;var-or-parens&gt; ::= x | ( &lt;expr&gt; )
</code></pre>
<p>We might worry that now, it is again possible to have an arbitrary expression as an argument to an operator, but the point is that, if we do this, it must be wrapped in parentheses, which ensures unambiguity.</p>
<blockquote>
<p><strong>Exercise.</strong> Give a leftmost derivation of <code>( x + x ) * x</code> in the above grammar. Draw its parse tree.</p>
</blockquote>
<blockquote>
<p><strong>Exercise.</strong> According to PEMDAS, we also need to handle exponentiation.
Give a grammar for arithmetic expressions including parentheses and exponentiation, using the following operator information.</p>
<div class="table-wrapper"><table><thead><tr><th>Operator</th><th>Fixity</th><th>Associativity</th><th>Precedence</th></tr></thead><tbody>
<tr><td><code>^</code></td><td>infix</td><td>right</td><td>3</td></tr>
<tr><td><code>*</code></td><td>infix</td><td>left</td><td>2</td></tr>
<tr><td><code>/</code></td><td>infix</td><td>left</td><td>2</td></tr>
<tr><td><code>+</code></td><td>infix</td><td>left</td><td>1</td></tr>
<tr><td><code>-</code></td><td>infix</td><td>left</td><td>1</td></tr>
</tbody></table>
</div></blockquote>
<blockquote>
<p><strong>Exercise.</strong> Write a grammar for Boolean expressions in Python.
You can check what sorts of expressions are allowed by using the Python interpreter.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="extended-bnf"><a class="header" href="#extended-bnf">Extended BNF</a></h1>
<p>It's not a secret that BNF specification are not always nice to write down.
There were quite a few hoops to jump through in order to design an unambiguous grammar for arithmetic expressions in the previous section.</p>
<p>So it's not surprising that there are a number of extensions to the BNF (meta-)syntax which make it more usable.
If you go digging around the Internet for Extended BNF (EBNF), you'll find a couple definitions, most of which are more complex than what we will choose to call EBNF here.
That said, the extensions we consider in this short section will a useful precursor for the following section on regular grammars and regular expressions.</p>
<h2 id="optional"><a class="header" href="#optional">Optional</a></h2>
<p>We use <code>[ SENT-FORM ]</code> to notate part of a production rule which is optional.
We may, for instance, want to write a language which has both if-then and if-then-else expressions.
Rather than using the BNF production rules</p>
<pre><code>&lt;if-expr&gt; ::= if &lt;expr&gt; then &lt;expr&gt; | if &lt;expr&gt; then &lt;expr&gt; else &lt;expr&gt;
</code></pre>
<p>we can use the EBNF production rule</p>
<pre><code>&lt;if-expr&gt; ::= if &lt;expr&gt; then &lt;expr&gt; [else &lt;expr&gt;]
</code></pre>
<p>These two rules express the same thing.
This also points to an important point: all BNF production rules are also EBNF production rules, and all EBNF production rules can be rewritten as a collection of BNF production rules.</p>
<blockquote>
<p><strong>Exercise.</strong> Rewrite the following EBNF production rule as a collection of BNF production rules.</p>
<pre><code>&lt;a&gt; ::= a [ &lt;b&gt; ] [ a ]
</code></pre>
</blockquote>
<blockquote>
<p><strong>Remark.</strong> One issue with extending BNF is now we've made it harder to express grammars which include the symbols used for the EBNF extensions (e.g., <code>[</code> and <code>]</code>).
In practice this is not a problem, we will try to be very explicit if something like <code>[</code> appears as part of a symbol of the grammar, and not a part of the specification of the grammar</p>
</blockquote>
<h2 id="alternative"><a class="header" href="#alternative">Alternative</a></h2>
<p>We use <code>( SENT-FORM-1 | SENT-FORM-2 | ... | SENT-FORM-k )</code> to notate the choice of multiple sentential forms as part of a production rule.
In the previous section we defined a grammar for arithmetic expressions with multiple rules for the choice of operation.</p>
<pre><code>&lt;expr&gt;          ::= &lt;only-mul-div&gt; | &lt;expr&gt; &lt;add-sub&gt; &lt;only-mul-div&gt;
&lt;only-mul-div&gt;  ::= &lt;var-or-parens&gt; | &lt;only-mul-div&gt; &lt;mul-div&gt; &lt;var-or-parens&gt;
&lt;add-sub&gt;       ::= + | -
&lt;mul-div&gt;       ::= * | /
&lt;var-or-parens&gt; ::= x | ( &lt;expr&gt; )
</code></pre>
<p>We can simplify this in EBNF by removing the production rules for operators.</p>
<pre><code>&lt;expr&gt;          ::= &lt;only-mul-div&gt; | &lt;expr&gt; (+ | -)  &lt;only-mul-div&gt;
&lt;only-mul-div&gt;  ::= &lt;var-or-parens&gt; | &lt;only-mul-div&gt; (* | /) &lt;var-or-parens&gt;
&lt;var-or-parens&gt; ::= x | ( &lt;expr&gt; )
</code></pre>
<h2 id="repetition"><a class="header" href="#repetition">Repetition</a></h2>
<p>We use <code>{ SENT-FORM }</code> to notate a part of a production rule which can be repeated as many times as we want.
We can also combine this with the alternative notation to as</p>
<pre><code>{SENT-FORM-1 | SENT-FORM-2 | ... | SENT-FORM-k}
</code></pre>
<p>to represent a collection of choices we may repeat as many times as we want.</p>
<p>In the same grammar as above, we can rewrite the recursive production rules in terms of repetition.</p>
<pre><code>&lt;expr&gt;          ::= &lt;only-mul-div&gt; { (+ | -) &lt;only-mul-div&gt; }
&lt;only-mul-div&gt;  ::= &lt;var-or-parens&gt; { (* | /) &lt;var-or-parens&gt; }
&lt;var-or-parens&gt; ::= x | ( &lt;expr&gt; )
</code></pre>
<p>In general, we can think of the following pairs of rules as equivalent.</p>
<pre><code>&lt;a&gt; ::= a { b }
&lt;a&gt; ::= a | &lt;a&gt; b

&lt;a&gt; ::= { b } a
&lt;a&gt; ::= a | b &lt;a&gt;
</code></pre>
<blockquote>
<p><strong>Exercise.</strong> Write an EBNF specification for Boolean expressions in Python.</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="regular-grammars-and-regular-expressions"><a class="header" href="#regular-grammars-and-regular-expressions">Regular Grammars and Regular Expressions</a></h1>
<p>Before moving onto parsing, we take one detour through as class of grammars that is <em>weaker</em> than context-free grammars (the grammars described by BNF specifications) but surprisingly useful.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>(right linear) regular grammar</strong> is one given by a BNF specification with the only the following kinds of rules.</p>
<ul>
<li><code>&lt;non-term&gt; ::= term</code></li>
<li><code>&lt;non-term&gt; ::= term | &lt;other-non-term&gt;</code> where <code>&lt;other-non-term&gt;</code> may be the same as <code>&lt;non-term&gt;</code></li>
<li><code>&lt;non-term&gt; ::= EMPTY</code> where <code>EMPTY</code> is a special symbol representing the empty sequence.</li>
</ul>
</blockquote>
<p>It is not difficult to imagine what a left linear regular grammar would be, but we won't be too concerned with this.
Here is a simple example of a regular grammar.</p>
<pre><code>&lt;s&gt; ::= a&lt;s&gt;
&lt;s&gt; ::= b&lt;a&gt;
&lt;a&gt; ::= EMPTY
&lt;a&gt; ::= c&lt;a&gt;
</code></pre>
<p>One key feature of regular grammars is that their derivations are very nicely structured.
At each step, there is always a single non-terminal symbol in the rightmost position which is either shifted over by a terminal symbol or dropped (i.e., replaced with <code>EMPTY</code>).</p>
<pre><code>&lt;s&gt;
a&lt;s&gt;
aa&lt;s&gt;
aaa&lt;s&gt;
aaab&lt;a&gt;
aaabc&lt;a&gt;
aaabcc&lt;a&gt;
aaabccc&lt;a&gt;
aaabcccc&lt;a&gt;
aaabcccc
</code></pre>
<blockquote>
<p><strong>Exercise.</strong> Give a derivation of <code>abcc</code> in the above grammar.
Is <code>b</code> a sentence of the above grammar?</p>
</blockquote>
<p>Another key feature is that they have a very compact representation, which can be seen to parallel the extensions to BNF we gave in the previous section.</p>
<blockquote>
<p><strong>Definition.</strong> A <strong>regular expression</strong> is defined as follows.</p>
<ul>
<li>A terminal symbol is a regular expression.</li>
<li>If <code>e</code> and <code>f</code> are regular expressions, then so are
<ul>
<li><code>(e|f)</code> (alternative)</li>
<li><code>(ef)</code> (sequence)</li>
<li><code>e*</code> (zero-or-more repetition, a.k.a., the Kleene star)</li>
</ul>
</li>
</ul>
<p>A regular expression <code>e</code> <strong>recognizes</strong> sentences according to the following rules.</p>
<ul>
<li>A terminal symbol <code>term</code> recognizes the the sentence <code>term</code>.</li>
<li>if <code>e</code> recognizes <code>s</code>, then <code>e|f</code> recognizes <code>s</code>.</li>
<li>if <code>f</code> recognizes <code>s</code>, then <code>e|f</code> recognizes <code>s</code>.</li>
<li>if <code>e</code> recognizes <code>s</code> and <code>f</code> recognizes <code>t</code>, then <code>(ef)</code> recognizes <code>st</code></li>
<li><code>e*</code> recognizes <code>EMPTY</code></li>
<li>if <code>e</code> recognizes <code>s_1</code>, <code>s_2</code>,...,<code>s_k</code> then <code>e*</code> recognizes <code>s_1s_2...s_k</code></li>
</ul>
</blockquote>
<p>We're not going to talk about the translation between these two representations, but it is a theorem of formal language theory that <em>every regular grammar can be represented regular expression and vice versa</em>.
This is to say, given a regular grammar, we can find a regular expression which recognizes the same sentences (and vice versa).</p>
<blockquote>
<p><strong>Example.</strong> We might notice that the above grammar has the property that it recognizes any sentence which is made up of any number of <code>a</code>s followed by a single <code>b</code> followed by any number of <code>c</code>s.
This is represented by the regular expression <code>(a*bc*)</code>.
We will often drop outer parentheses, so that we could also write the regular expression <code>a*bc*</code>.</p>
</blockquote>
<blockquote>
<p><strong>Example.</strong> The regular expression <code>a((bc)|(cb))*d</code> recognizes the sentence <code>abccbd</code>.
This sentence contains a single <code>a</code>, followed by to repetitions of either <code>bc</code> or <code>cb</code> (the first <code>bc</code>, the second <code>cb</code>), followed by a single <code>d</code>.</p>
</blockquote>
<h2 id="a-more-interesting-example-urls"><a class="header" href="#a-more-interesting-example-urls">A More Interesting Example: URLs</a></h2>
<p><img src="images/url.png" alt="URL Example" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parsing"><a class="header" href="#parsing">Parsing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="grammars-as-adts"><a class="header" href="#grammars-as-adts">Grammars as ADTs</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="parser-combinators"><a class="header" href="#parser-combinators">Parser Combinators</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="formal-semantics"><a class="header" href="#formal-semantics">Formal Semantics</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operational-semantics"><a class="header" href="#operational-semantics">Operational Semantics</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="variables"><a class="header" href="#variables">Variables</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="scoping"><a class="header" href="#scoping">Scoping</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="binding"><a class="header" href="#binding">Binding</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="subprograms"><a class="header" href="#subprograms">Subprograms</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="activation-records"><a class="header" href="#activation-records">Activation Records</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="activation-stack"><a class="header" href="#activation-stack">Activation Stack</a></h1>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
